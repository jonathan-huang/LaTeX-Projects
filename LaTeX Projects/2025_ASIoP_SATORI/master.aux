\relax 
\providecommand{\transparent@use}[1]{}
\providecommand\zref@newlabel[2]{}
\abx@aux@refcontext{anyt/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to Machine Learning}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {note}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Note\thmtformatoptarg {Machine Learning in the Eye of a Mathematician}}{2}{thmt@dummyctr.dummy.1}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{4736274}{28176420}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Models of Machine Learning}{2}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Support-Vector Machines}{2}{subsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Regression Analysis}{2}{subsection.1.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Decision Trees}{2}{subsection.1.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Enter Caption}}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mlp}{{1.1}{3}{Enter Caption}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Genetic Algorithms}{3}{subsection.1.1.4}\protected@file@percent }
\newlabel{ssec:NN}{{1.1.5}{3}{Neural Networks}{subsection.1.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Neural Networks}{3}{subsection.1.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.6}Perceptron and Multi-Layer Perceptron}{3}{subsection.1.1.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Diagram of a typical neuron in biology.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:neuron}{{1.2}{4}{Diagram of a typical neuron in biology}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.7}Activation Functions}{4}{subsection.1.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.8}Backpropagation}{4}{subsection.1.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}What is Learning?}{4}{section.1.2}\protected@file@percent }
\abx@aux@cite{0}{HGloss}
\abx@aux@segm{0}{0}{HGloss}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Learning Paradigms}{5}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Supervised Learning}{5}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Unsupervised Learning}{5}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Reinforcement Learning}{5}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Training and Cross-Validation}{5}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Overfitting and Underfitting}{5}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Loss}{5}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Mean Square Error}{5}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Cross Entropy Loss}{5}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Gaussian Loss}{5}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Heteroscedastic Gaussian Loss}{5}{subsection.1.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Latency}{6}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Bias-Variance Tradeoff}{6}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}James-Stein Estimator}{6}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}State of the Art of Machine Learning}{6}{section.1.6}\protected@file@percent }
\abx@aux@cite{0}{dl}
\abx@aux@segm{0}{0}{dl}
\abx@aux@cite{0}{McCulloch1943}
\abx@aux@segm{0}{0}{McCulloch1943}
\abx@aux@cite{0}{mangh2020threshold}
\abx@aux@segm{0}{0}{mangh2020threshold}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Deep Learning}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}What is Deep Learning}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Historical Development}{7}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Diagram of a McCulloch-Pitts neuron}}{7}{figure.caption.7}\protected@file@percent }
\newlabel{fig:mcp_neuron}{{2.1}{7}{Diagram of a McCulloch-Pitts neuron}{figure.caption.7}{}}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~2.1.1\else \numberline {2.1.1}Definition\fi \thmtformatoptarg {Units}}{8}{definition.2.1.1}\protected@file@percent }
\pgfsyspdfmark {pgfid2}{4736274}{49166047}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~2.1.2\else \numberline {2.1.2}Definition\fi \thmtformatoptarg {Deep Learning}}{8}{definition.2.1.2}\protected@file@percent }
\pgfsyspdfmark {pgfid3}{4736274}{41561279}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~2.1.1\else \numberline {2.1.1}Theorem\fi \thmtformatoptarg {Universal Approximation Theorem}}{8}{theorem.2.1.1}\protected@file@percent }
\pgfsyspdfmark {pgfid4}{4736274}{38677378}
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Remark}{8}{thmt@dummyctr.dummy.5}\protected@file@percent }
\pgfsyspdfmark {pgfid5}{4736274}{36596549}
\@writefile{loe}{\contentsline {eg}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Example\thmtformatoptarg {A neural network corresponds to a function mapping}}{8}{thmt@dummyctr.dummy.6}\protected@file@percent }
\pgfsyspdfmark {pgfid6}{4736274}{8256460}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}End-to-End Learning}{9}{subsection.2.1.2}\protected@file@percent }
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~2.1.3\else \numberline {2.1.3}Definition\fi \thmtformatoptarg {Negative Log Likelihood and Entropy}}{9}{definition.2.1.3}\protected@file@percent }
\pgfsyspdfmark {pgfid7}{4736274}{22187648}
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Remark\thmtformatoptarg {KL Divergence}}{9}{thmt@dummyctr.dummy.8}\protected@file@percent }
\pgfsyspdfmark {pgfid8}{4736274}{13485600}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Recent Trends}{9}{section.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The kernel is a fixed pattern of weights that convolutes with every region of the input image.}}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:cnn_intro}{{2.2}{10}{The kernel is a fixed pattern of weights that convolutes with every region of the input image}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The kernel is a fixed pattern of weights that convolutes with every region of the input image.}}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:kernel}{{2.3}{10}{The kernel is a fixed pattern of weights that convolutes with every region of the input image}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convolutional Neural Networks}{10}{section.2.3}\protected@file@percent }
\@writefile{loe}{\contentsline {note}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Note\thmtformatoptarg {Rule of Thumb 1}}{10}{thmt@dummyctr.dummy.9}\protected@file@percent }
\pgfsyspdfmark {pgfid9}{4736274}{22088493}
\@writefile{loe}{\contentsline {note}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Note\thmtformatoptarg {Rule of Thumb 2}}{10}{thmt@dummyctr.dummy.10}\protected@file@percent }
\pgfsyspdfmark {pgfid10}{4736274}{19204592}
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Remark}{10}{thmt@dummyctr.dummy.11}\protected@file@percent }
\pgfsyspdfmark {pgfid11}{4736274}{15550899}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Introducing the Kernel}{10}{subsection.2.3.1}\protected@file@percent }
\@writefile{loe}{\contentsline {eg}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Example\thmtformatoptarg {1D Convolutional Neural Network}}{10}{thmt@dummyctr.dummy.12}\protected@file@percent }
\pgfsyspdfmark {pgfid13}{4736274}{5186335}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Illustrating the working principles of a DNN kernel with a one-dimensional CNN.}}{11}{figure.2.4}\protected@file@percent }
\newlabel{fig:1d_conv}{{2.4}{11}{Illustrating the working principles of a DNN kernel with a one-dimensional CNN}{figure.2.4}{}}
\pgfsyspdfmark {pgfid14}{4736274}{36345581}
\@writefile{loe}{\contentsline {eg}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Example\thmtformatoptarg {An Example with Numbers}}{11}{thmt@dummyctr.dummy.13}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Numeric example of 1D convolutional network.}}{11}{figure.2.5}\protected@file@percent }
\newlabel{fig:1d_conv_num}{{2.5}{11}{Numeric example of 1D convolutional network}{figure.2.5}{}}
\pgfsyspdfmark {pgfid16}{4736274}{5050708}
\pgfsyspdfmark {pgfid17}{4736274}{38209605}
\@writefile{loe}{\contentsline {note}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Note\thmtformatoptarg {Computation for CNNs: GPU vs. TPU}}{12}{thmt@dummyctr.dummy.14}\protected@file@percent }
\pgfsyspdfmark {pgfid18}{4736274}{32964133}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}CNN is Like a Visual Cortex}{12}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The receptive field in a 2D convolutional network is shown.}}{12}{figure.2.6}\protected@file@percent }
\newlabel{fig:receptive_field}{{2.6}{12}{The receptive field in a 2D convolutional network is shown}{figure.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Pooling and Downsampling}{12}{subsection.2.3.3}\protected@file@percent }
\@writefile{loe}{\contentsline {eg}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Example\thmtformatoptarg {Tensor Operations}}{12}{thmt@dummyctr.dummy.15}\protected@file@percent }
\pgfsyspdfmark {pgfid20}{4736274}{4736274}
\pgfsyspdfmark {pgfid21}{4736274}{46401660}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Batch Renormalization}{13}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Regularization}{13}{subsection.2.3.5}\protected@file@percent }
\@writefile{loe}{\contentsline {note}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Note}{13}{thmt@dummyctr.dummy.16}\protected@file@percent }
\pgfsyspdfmark {pgfid22}{4736274}{22452482}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Dropout}{13}{subsection.2.3.6}\protected@file@percent }
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap \else \numberline {\let \autodot \@empty }\fi Remark}{13}{thmt@dummyctr.dummy.17}\protected@file@percent }
\pgfsyspdfmark {pgfid23}{4736274}{8792819}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Large Language Models}{14}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Attention Mechanism}{14}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Generative Pretrained Transformers}{14}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Small Language Models}{14}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Graph Neural Networks}{15}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Graphs}{15}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Graph Neural Networks}{15}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Graph Convolutional Networks}{15}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Message-Passing Neural Networts}{15}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Graph Attention Networks}{15}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Summary}{15}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Coding in the Age of AI}{16}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Vibe Coding}{16}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}What is vibe coding?}{16}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Context Engineering}{16}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}ChatGPT and Where It Is Going}{16}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Context Engineering is not Prompt Engineering}{16}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Some Warnings}{16}{section.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Context engineering diagram from https://blog.langchain.com/the-rise-of-context-engineering/}}{17}{figure.caption.11}\protected@file@percent }
\newlabel{fig:context}{{5.1}{17}{Context engineering diagram from https://blog.langchain.com/the-rise-of-context-engineering/}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces A schematic diagram of the relation between various ChatGPT models by X user @arithmoquine.}}{17}{figure.caption.12}\protected@file@percent }
\newlabel{fig:gpt_evo}{{5.2}{17}{A schematic diagram of the relation between various ChatGPT models by X user @arithmoquine}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces A graph showing the exponential increase in task length achievable by AI agents over time.}}{18}{figure.caption.13}\protected@file@percent }
\newlabel{fig:task_length}{{5.3}{18}{A graph showing the exponential increase in task length achievable by AI agents over time}{figure.caption.13}{}}
\abx@aux@read@bbl@mdfivesum{D08C8A45CB13A1B4C7318AF3DEAAE7D6}
\abx@aux@defaultrefcontext{0}{dl}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{HGloss}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mangh2020threshold}{anyt/global//global/global/global}
\abx@aux@defaultrefcontext{0}{McCulloch1943}{anyt/global//global/global/global}
\gdef \@abspage@last{21}
