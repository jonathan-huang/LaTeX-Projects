\chapter{Deep Learning}
% \lecture{4}{9 Sep. 08:00}{Fourth Lecture}

\section{What is Deep Learning}
This section will use \cite{dl} and () as the main reference.

\textbf{Deep learning (DL)} is the study of \emph{hierarchically-structured} function approximators known as \emph{neural networks}, which was introduced in subsection \ref{ssec:NN}. 

\subsection{Historical Development}

The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks (Ivakhnenko \& Lapa, 1965). The first deep learning multilayer perceptron trained by stochastic gradient descent (SDG) was published in 1967 by Shun'ichi Amari.

In a deep neural network, the circuits, or the neural networks, usually have many layers. In 1943, Warren McCulloch and Walter Pitts formulated the \textbf{McCulloch-Pitts (MCP) neuron} in their paper "\textit{A logical calculus of the ideas immanent in nervous activity}" \cite{McCulloch1943}, shown in figure (\ref{fig:mcp_neuron}). The MCP neuron is a restricted artificial neuron introduced during the formative years of ANNs (1943 - 1958) as a way to model biological networks in the brain. Nonlinearities were modeled by introducing a threshold logic function, with inhibitory and excitatory inputs and bias.

In 1958, \textbf{Franklin Rosenblatt} introduced a major advancement called the \textbf{perceptron}, now regarded as the first generation neural networks \cite{mangh2020threshold}. Based on the MCPs neuron and the findings of Canadian psychologist \textbf{Donal O. Hebb}, Rosenblatt developed the first perceptron, with learnable weight and bias values that could be determined analytically or by a \textit{learning algorithm}. Rosenblatt's perceptrons paved the way for the development of ANNs, which progressed into deep learning with its applications in AI.

The Rosenblatt Perceptron consisted of a single-layer MCP model neuron which sums up all the weighted inputs and produces a binary output using a threshold activation function. Such networks and their variations are collectively called perceptrons. Firthermore, Rosenblatt proved that if the input vectors representing two linearly separable classes are used to train the model neuron, the perceptron learning algorithm will always converge. 

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/mcp.png}
    \caption{Diagram of a McCulloch-Pitts neuron}
    \label{fig:mcp_neuron}
\end{figure}

The real reason for the success of deep learning has not yet been elucidated. It is plausible that the long computation paths present in a DNN allow input variables to interact in complex ways. 

\begin{definition}[Units]
    Each node in a neural network is also called a \textit{unit}.
\end{definition}

\begin{definition}[Deep Learning]
    We can define deep learning (DL) as follows:
    \begin{itemize}
        \item We call a model a deep model if back-propagation is required to train it.
        \item A network is composed of layers of nodes, normally a neural network is called a deep neural network (DNN) if it has many layers. 
        \item  Deep learning is a subset of machine learning that is based on deep models, and learns using a deep neural network. 
    \end{itemize}
\end{definition}

\begin{theorem}[Universal Approximation Theorem]
    A network with two or more layers and at least one nonlinear unit can be utilized to approximate any continuous function. 
\end{theorem}
\begin{remark}
    A deep learning model is a function approximator.
\end{remark}
The above remark can be illustrated with the folowing example.

\begin{eg}[A neural network corresponds to a function mapping]
    The value of the $j$-th unit in the $i$-th layer, where the input layer is labelled with $i=0$, is represented with $x^{(i)}_{j}$. Then in the first layer, we have
    % insert graphics
    \begin{equation}
        \begin{split}
            x^{(1)}_1 &= g^{(1)}_{1} \left(w^{(1)}_{11}x^{(0)}_{1} + w^{(1)}_{21}x^{(1)}_{2} + b^{(1)}_{1}\right) = g^{(1)}_{1}\left(\mathbf{w}^{(1)}_{1}\cdot \mathbf{x}^{(0)} + b^{(1)}_{1}\right), \\
            x^{(1)}_2 &= g^{(1)}_{2} \left(w^{(1)}_{12}x^{(0)}_{1} + w^{(1)}_{22}x^{(1)}_{2} + b^{(1)}_{2}\right) = g^{(1)}_{2}\left(\mathbf{w}^{(1)}_{2}\cdot \mathbf{x}^{(0)} + b^{(1)}_{2}\right).
        \end{split}
    \end{equation}
    This can be collected into a clean vector equation:
    \begin{equation}
        \mathbf{x}^{(1)} = \mathbf{g}^{(1)}\left(\mathbf{w}^{(1)}\cdot\mathbf{x}^{(0)} + \mathbf{b}^{(1)}\right).
    \end{equation}
    For sake of clarity, we will use normal math font instead of bold font to represent these vectors. This gives 
    \begin{equation}
        \begin{split}
            x^{(1)} &= g^{(1)}\left(w^{(1)}\cdot x^{(0)} + b^{(1)}\right), \\
            x^{(2)} &= g^{(2)}\left(w^{(2)}\cdot x^{(1)} + b^{(2)}\right) \\
            &= g^{(2)}\left(w^{(2)}\cdot g^{(1)}\left(w^{(1)}\cdot x^{(0)} + b^{(1)}\right) + b^{(2)}\right).
        \end{split}
    \end{equation}

    Writing the input layer $x^{(0)}$ simply as $x$, and 
    this network corresponds to the map
    \begin{equation}
        h_W(x) = g^{(2)}\left(w^{(2)}\cdot g^{(1)}\left(w^{(1)}\cdot x + b^{(1)}\right) + b^{(2)}\right).
    \end{equation}

    We generalize the above discussion to more complex DNNs: the collection of weights can be organized as a tensor
    \begin{equation}
        W = \begin{pmatrix}
            w^{(1)} & w^{(2)} & \cdots & w^{(m)}
        \end{pmatrix}^{T} \in M_{m\times n}(\mathbb{R}),
    \end{equation}
    the bias is a vector $b^{(i)} \in \mathbb{R}^m$, and the recursive relationship between neurons $x^{(i-1)} \in [0,1]^n$ and $x^{(i)} \in [0,1]^m$ of neighboring layers is
    \begin{equation}
        x^{(i)} = g^{(i)}\left(w^{(i)}\cdot x^{(i-1)} + b^{(i)}\right).
    \end{equation}
\end{eg}

\subsection{End-to-End Learning}
Many times, a complex computational syste, for some task is composed of several trainable subsystems, so it is helpful to train these subsystems and connect their output to the input of the next subsystem, reducing training capacity requirements. There are important distinctions between each of the three important components of a DNN: \textbf{input}, \textbf{output}, and \textbf{hidden layers}. 
\begin{enumerate}
    \item Input layer: This layer encodes training or testing data into numerical values to be associated with input units.
    
    \item Output layer: Ideally, the loss for output units would be zero or close to zero. However, this is almost never the case for randomly chosen parameters, so we have to adjust these parameters using backpropagation and have the quality of parameter estimation after each application evaluated with some loss function. A common loss function is the \textbf{negative log likelihood}.
    
    \item Hidden layer: During the years 1985 to 2010, the sigmoid and $\tanh$ functions were exclusively used for the hidden layers, while from 2010 onwards $\mathrm{ReLU}$ and $\mathrm{softplus}$ are used. It is observed (but not yet rigorously explained) that \textit{deep and narrow networks} usually have better learning ability compared with shallow and wide networks when the total number of weights is kept fixed.
\end{enumerate}

\begin{definition}[Negative Log Likelihood and Entropy]
    Let $x_j$ be the $N$ examples given as input to the DNN, and $y_j$ the respective true value. We shall minimize the function
    \begin{equation}
        - \sum^{N}_{j} \log\left[P_W\left(y_j \middle| x_j \right)\right].
    \end{equation}
    This is an approximation to the \textbf{cross entropy loss}, defined over two probability distributions $P$, $Q$ and given by
    \begin{equation}
        H(P,Q) = \mathbb{E}_{z \sim P(z)}\left[\log Q(z) \right] = \int \mathrm{d}z\, P(z) \log Q(z).
    \end{equation}
    Minimizing the cross entropy loss of our probability $P_W$ would be equivalent to minimizing
    \begin{equation}
        H\left(P^*(x,y), P_W\left(y \middle| x\right)\right),
    \end{equation}
    but it is impossible to know the true distribution \textit{a priori} (otherwise there won't be so much trouble!), so we go with the previously mentioned approximation.
\end{definition}

\begin{remark}[KL Divergence]
    The cross entropy of two probability distributions is not a distance function (metric) in the space of probability distributions. This can be seen by the fact that 
    \begin{equation}
        H(P,P) = \int \mathrm{d}z \, P(z) \log P(z) \equiv H(P) \neq 0.
    \end{equation}
    Here $H(P)$ is the familiar Shannon entropy. However, we \textit{can} define a metric by subtracting off the nonzero part, which gives the definition of the \textbf{Kullback-Leibler divergence}:
    \begin{equation}
        D_{\text{KL}}\left( P \,||\, Q\right) \equiv H(P,Q) - H(P).
    \end{equation}
\end{remark}

\section{Recent Trends}
In the 2025 COLT (Conference On Learning Theory, one of the most prominent pure theory conferences in the field of ML), the top five most popular research categories are, neural networks/deep learning, online learning, bandit problems, reinforcement learning, and high-dimensional statistics. This is taken from Li Yen-Huan's Facebook post from 2025 July.

\section{Convolutional Neural Networks}

\textbf{Convolutional Neural Networks (CNNs)} are a special type of deep neural networks (DNN) consisting of three main steps: \textbf{convolutional kernel}, \textbf{pooling}, and \textbf{vetorization}. CNNs are also one of the first ANNs that closely mimic the biological working of \textit{visual neurons}, and have become an essential tool for computer vision.

In order to classify images, we want to follow the following rules of thumb:
\begin{note}[Rule of Thumb 1]
    Construct the first hidden layer such that each hidden layer unit receives input from a small local region. That is, let each local region possess $l \ll n$ pixels, then the total would be $N = ln \ll n^2$.
\end{note}

\begin{note}[Rule of Thumb 2]
    We expect image data to exhibit approximate spatial invariance, meaning that the image of an object translated by some distance is the same distance. 
\end{note}

\begin{remark}
    The above two rules of thumb do not capture the \textit{rotational invariance} of an object, hence any rotation of an image of a dog may render it completely unrecognizable to the algorithm. To solve this problem, we can naively give the CNN a training set containing rotated images.  
\end{remark}

By injecting some prior knowledge of adjacency and spatial invariance, we can develop models with far fewer parameters that can recognize images.

\subsection{Introducing the Kernel}
The \textbf{kernel} is a small local region that sweeps over the image. This ensures translational invariance is obeyed by duplicating the same node structure over the entire image. Refer to figure (\ref{fig:kernel}), suppose as before that the 2D image contains $n$ pixels, the kernel contains $l$ pixels and has $d$ hidden units with distinct weights. In total there are $dl$ weights, a number independent of the image size! We can understand the kernel as the \textit{fixed pattern of weights} that scans through the input images.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/cnn_intro.png}
    \caption{The kernel is a fixed pattern of weights that convolutes with every region of the input image.}
    \label{fig:cnn_intro}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{Figures/kernel.jpg}
    \caption{The kernel is a fixed pattern of weights that convolutes with every region of the input image.}
    \label{fig:kernel}
\end{figure}

\begin{eg}[1D Convolutional Neural Network]
    The idea of a CNN kernel is best illustrated with a 1D problem. Supppose there are $n=8$ units in the input layer, and the kernel consists of $l=3$ units, with stride $s=1$. Then the network produces  

    \vspace{1em}
    \phantomsection % this gives better hyperlink behavior
    \begin{center}
        \begin{tikzpicture}[>=Latex, font=\small]
        % ===== Parameters =====
        \def\N{8}  % n: number of input pixels
        \def\L{3}   % l: kernel length
        \pgfmathtruncatemacro{\O}{\N-\L+1} % output length (valid conv)
        
        % ===== Styles =====
        \tikzset{
          pix/.style={draw, minimum width=6mm, minimum height=6mm, align=center},
          ker/.style={draw, fill=blue!8, minimum width=6mm, minimum height=6mm, align=center},
          win/.style={draw=blue, rounded corners=2pt, very thick},
          outnode/.style={draw, circle, minimum size=6mm, align=center}, % renamed
          note/.style={font=\footnotesize, inner sep=1pt},
        }
        
        % ===== Input row (n pixels) =====
        \path (0,0) coordinate (I0);
        \foreach \i in {1,...,\N} {
          \node[pix] (x\i) at ($(I0)+({(\i-1)*0.8},0)$) {$x_{\i}$};
        }
        
        % Brace for n
        \draw[decorate,decoration={brace,amplitude=5pt}]
          ($(x1.south west)+(0,-0.25)$) -- ($(x\N.south east)+(0,-0.25)$)
          node[midway,below=6pt,note] {$n$ input pixels};
        
        % ===== Kernel (l pixels), aligned over first l inputs =====
        \path ($(x1.north west)+(0,1.2)$) coordinate (K0);
        \foreach \j in {1,...,\L} {
          \node[ker] (w\j) at ($(K0)+({(\j-1)*0.8},0)$) {$w_{\j}$};
        }
        
        % Brace for l
        \draw[decorate,decoration={brace,amplitude=5pt}]
          ($(w1.north west)+(0,0.25)$) -- ($(w\L.north east)+(0,0.25)$)
          node[midway,above=6pt,note] {$l$ kernel pixels};
        
        % Window on input (first l)
        \draw[win] ($(x1.north west)+(-0.05,0.05)$) rectangle ($(x\L.south east)+(0.05,-0.05)$);
        
        % Connection lines (elementwise multiply hint)
        \foreach \j in {1,...,\L} {
          \draw[blue!60] (w\j.south) -- ($(x\j.north)$);
        }
        
        % First output (dot product result)
        \path ($(x1.south)!0.5!(x\L.south)$) coordinate (midwin);
        \node[outnode] (y1) at ($(midwin)+(0,-1.2)$) {$y_{1}$};
        \draw[->,thick] ($(x1.south west)!0.5!(x\L.south east)$) -- (y1.north)
          % node[midway,right=2pt,note] {dot product};
          node[midway,right=2pt,note] {};
        
        % ===== Output row (n-l+1 pixels) =====
        \foreach \k in {1,...,\O} {
          % place outputs evenly under corresponding windows
          \pgfmathtruncatemacro{\start}{\k}
          \pgfmathtruncatemacro{\stop}{\k+\L-1}
          \path ($(x\start.south)!0.5!(x\stop.south)$) coordinate (oc\k);
          \ifnum\k=1\relax
            % already defined y1
          \else
            \node[outnode] (y\k) at ($(oc\k)+(0,-1.2)$) {$y_{\k}$};
          \fi
        }
        
        % Brace for output length
        \draw[decorate,decoration={brace,amplitude=5pt}]
          ($(y1.south west)+(-0.25,-0.25)$) -- ($(y\O.south east)+(0.25,-0.25)$)
          node[midway,below=6pt,note] {$n-l+1$ outputs};
        
        % ===== Slide indication =====
        % ghost kernel at next position
        \foreach \j [evaluate=\j as \jj using int(\j+1)] in {1,...,\L} {
          \draw[rounded corners=1pt, blue!30]
            ($(w\j.north west)+(0.8,0)$) rectangle ($(w\j.south east)+(0.8,0)$);
        }
        \draw[->,blue!60] ($(w\L.east)+(0,0.0)$) -- ++(0.6,0)
          node[midway,above,note] {slide};
        
        % Dots to indicate repeating structure
        \node[note] at ($(y1)!0.5!(y\O)+(0,-0.9)$) {$\cdots$};
        
        % ===== Legend (optional) =====
        \node[draw,rounded corners=2pt,align=left,anchor=west,fill=none,text opacity=1, note]
        at ($(x\N.east)+(1.3,0.1)$) {
        \textbf{1D convolution}\\
        $y_k=\displaystyle\sum_{j=1}^{l} w_j\, x_{k+j-1}$\quad for $k=1,\dots,n-l+1$
        };
        
        \end{tikzpicture}
        \captionof{figure}{Illustrating the working principles of a DNN kernel with a one-dimensional CNN.}
        \label{fig:1d_conv}
    \end{center}
\end{eg}

\begin{eg}[An Example with Numbers]
    Consider the 1D CNN problem with kernel size $l=3$ and stride $s=2$ in the following figure. Suppose the values in the nodes represent how bright the pixel is, then the specific kernel we have chosen picks out region with low intensity, i.e. the dark spots in the image. As the result indicates, the central region with a $2$ in the middle returns the highest value ($9$) after convolution with the kernel.

    % insert figure
    \begin{center}
        \begin{tikzpicture}[
          ibox/.style={draw, minimum width=8mm, minimum height=8mm, align=center},
          obox/.style={draw, minimum width=8mm, minimum height=8mm, align=center},
          inode/.style={draw, circle, minimum size=8mm, inner sep=0pt},
          wlab/.style={font=\scriptsize, inner sep=1pt, fill=none},
          brace/.style={decorate, decoration={brace, amplitude=5pt}}
        ]
        
        % --- Input (squares) ---
        \foreach \v [count=\i] in {5,6,6,2,5,6,5}{
          \node[ibox] (x\i) at (0,-\i) {\v};
        }
        \draw[brace] ($(x7.south west)+(-3pt,-3pt)$) -- ($(x1.north west)+(-3pt,3pt)$)
          node[midway, left=6pt] {$\text{input } \mathbf{x}$};
        
        % --- Internal conv nodes (circles) ---
        \node[inode] (h1) at (3,-2) {};
        \node[inode] (h2) at (3,-4) {};
        \node[inode] (h3) at (3,-6) {};
        \node[above=2mm of h1] {stride $s=2$};
        
        % Connections to h1: x1,x2,x3 with weights +1,-1,+1
        % \draw[-stealth] (x1.east) -- (h1.west) node[wlab, pos=0.55, above] {$+1$};
        % \draw[-stealth] (x2.east) -- (h1.west) node[wlab, pos=0.55] {$-1$};
        % \draw[-stealth] (x3.east) -- (h1.west) node[wlab, pos=0.55, below] {$+1$};
        
        % Connections to h1: x1,x2,x3 with weights +1,-1,+1
        % Make a box around kernel labels
        \coordinate (p11) at ($(x1.east)!0.3!(h1.west)$); % before label
        \coordinate (p12) at ($(x2.east)!0.3!(h1.west)$);
        \coordinate (p13) at ($(x3.east)!0.3!(h1.west)$);
        \coordinate (q11) at ($(x1.east)!0.7!(h1.west)$); % after label
        \coordinate (q12) at ($(x2.east)!0.7!(h1.west)$);
        \coordinate (q13) at ($(x3.east)!0.7!(h1.west)$);
        
        \draw (x1.east) -- (p11);
        \node[wlab,above] (w11) at ($(p11)!0.5!(q11)$) {$+1$};
        \draw[-stealth] (q11) -- (h1.west);
        \draw (x2.east) -- (p12);
        \node[wlab] (w12) at ($(p12)!0.5!(q12)$) {$-1$};
        \draw[-stealth] (q12) -- (h1.west);
        \draw (x3.east) -- (p13);
        \node[wlab,below] (w13) at ($(p13)!0.5!(q13)$) {$+1$};
        \draw[-stealth] (q13) -- (h1.west);
        % Add a label called "kernel window"
        \node[
            draw,
            rounded corners=3pt,
            fit={(w11) (w12) (w13)},
            inner sep=4pt,
            label=above:{\shortstack{kernel\\window}}] {};
        
        % Connections to h2: x3,x4,x5
        % Make a box around kernel labels
        \coordinate (p21) at ($(x3.east)!0.3!(h2.west)$); % before label
        \coordinate (p22) at ($(x4.east)!0.3!(h2.west)$);
        \coordinate (p23) at ($(x5.east)!0.3!(h2.west)$);
        \coordinate (q21) at ($(x3.east)!0.7!(h2.west)$); % after label
        \coordinate (q22) at ($(x4.east)!0.7!(h2.west)$);
        \coordinate (q23) at ($(x5.east)!0.7!(h2.west)$);
        
        \draw (x3.east) -- (p21);
        \node[wlab,above] (w21) at ($(p21)!0.5!(q21)$) {$+1$};
        \draw[-stealth] (q21) -- (h2.west);
        \draw (x4.east) -- (p22);
        \node[wlab] (w22) at ($(p22)!0.5!(q22)$) {$-1$};
        \draw[-stealth] (q22) -- (h2.west);
        \draw (x5.east) -- (p23);
        \node[wlab,below] (w23) at ($(p23)!0.5!(q23)$) {$+1$};
        \draw[-stealth] (q23) -- (h2.west);
        \node[draw,rounded corners=3pt,fit={(w21) (w22) (w23)},inner sep=4pt] {};
        
        % Connections to h3: x5,x6,x7
        % Make a box around kernel labels
        \coordinate (p31) at ($(x5.east)!0.3!(h3.west)$); % before label
        \coordinate (p32) at ($(x6.east)!0.3!(h3.west)$);
        \coordinate (p33) at ($(x7.east)!0.3!(h3.west)$);
        \coordinate (q31) at ($(x5.east)!0.7!(h3.west)$); % after label
        \coordinate (q32) at ($(x6.east)!0.7!(h3.west)$);
        \coordinate (q33) at ($(x7.east)!0.7!(h3.west)$);
        
        \draw (x5.east) -- (p31);
        \node[wlab,above] (w31) at ($(p31)!0.5!(q31)$) {$+1$};
        \draw[-stealth] (q31) -- (h3.west);
        \draw (x6.east) -- (p32);
        \node[wlab] (w32) at ($(p32)!0.5!(q32)$) {$-1$};
        \draw[-stealth] (q32) -- (h3.west);
        \draw (x7.east) -- (p33);
        \node[wlab,below] (w33) at ($(p33)!0.5!(q33)$) {$+1$};
        \draw[-stealth] (q33) -- (h3.west);
        \node[draw,rounded corners=3pt,fit={(w31) (w32) (w33)},inner sep=4pt] {};
        
        % --- Outputs (squares) ---
        \node[obox, right=12mm of h1] (y1) {\bfseries 5};
        \node[obox, right=12mm of h2] (y2) {\bfseries 9};
        \node[obox, right=12mm of h3] (y3) {\bfseries 4};
        \draw[-stealth, thick] (h1) -- (y1);
        \draw[-stealth, thick] (h2) -- (y2);
        \draw[-stealth, thick] (h3) -- (y3);
        \node[right=4mm of y2] {$\text{output } \mathbf{y}$};
        
        % Kernel legend (internal nodes are circles)
        \node[inode, below right=8mm and -1mm of x7] (k1) {$+1$};
        \node[inode, right=2mm of k1] (k2) {$-1$};
        \node[inode, right=2mm of k2] (k3) {$+1$};
        \node[below=1mm of k2] {kernel $\mathbf{k}$};
        
        \end{tikzpicture}
        \captionsetup{hypcap=false}
        \captionof{figure}{Numeric example of 1D convolutional network.}
        \label{fig:1d_conv_num}
    \end{center}
    
    The \textbf{discrete convolution} can be written as 
    \begin{equation}
        z = x * k,
    \end{equation}
    which in fact means
    \begin{equation}
        z_i = \sum^{l}_{j=1} k_{j} x_{j+i-(l+1)/2} .
    \end{equation}

    Although not necessary, the above convolution process can be represented as matrix multiplication, with the kernel matrix $K$ constructed by translating the kernel $k$ and filling the remaining entries with zeroes:
    \begin{equation}
        \begin{pmatrix}
            1 & -1 & 1 & 0 & 0 & 0 & 0 \\
            0 & 0 & 1 & -1 & 1 & 0 & 0 \\
            0 & 0 & 0 & 0 & 1 & -1 & 1 \\
        \end{pmatrix}
        \begin{pmatrix}
            5 \\ 6 \\ 6 \\ 2 \\ 5 \\ 6 \\ 5
        \end{pmatrix}
        =
        \begin{pmatrix}
            5 \\ 9 \\ 4 \\
        \end{pmatrix}.
    \end{equation}
    Here the output is $\mathrm{out}(x) = (5, 9, 4)$. In higher-dimensional CNNs this equation is appropriately generalized into \textbf{tensor multiplication} \footnote{In ML lingo, a tensor just represents any tuple or high-dimensional array of numbers, and is distinct from tensor as defined in mathematics, which are basis-independent algebraic objects that describes a multilinear relationship between sets of objects living inside a vector space.}. 
\end{eg}

\begin{note}[Computation for CNNs: GPU vs. TPU]
    \textbf{Tensor Processing Unit (TPU)} is an application-specific integrated circuit (ASIC) developed by Google for NN learning, using Google's own TensorFlow software. When using stochastic gradient descent (SGD), we separate data into \textbf{minibatches} chosen randomly ("stochastically") at each step. Each training example in the SGD minibatch can be computed in parallel, so we can leverage the power of hardware parallelism with TPUs.
\end{note}

\subsection{CNN is Like a Visual Cortex}
CNNs were originally inspired by visual cortex models developed in neuroscience. A node in the $m$-th hidden layer has a \textbf{receptive field} of size $(l-1)m + 1$, and with stride $s$ the receptive field grows as $O(ls^m)$, indicating exponential growth with respect to depth.
% insert a diagram illustrating receptive field

\vspace{1em}

\begin{center}
    \centering
    \begin{tikzpicture}[font=\scriptsize, x=0.35cm, y=0.35cm,
      rf1/.style={fill=blue!18,draw=blue!60,thick},
      rf0/.style={fill=orange!18,draw=orange!70!black,thick},
      grid/.style={draw=black!60, line width=0.3pt},
      box/.style={draw=black, thick, rounded corners=2pt},
      lab/.style={font=\footnotesize}
    ]
    
    % Horizontal shifts
    \def\sA{0}      % Input
    \def\sB{16}     % Conv1
    \def\sC{28}     % Conv2
    
    % Sizes
    \def\nA{7}  % 7x7 input
    \def\nB{5}  % 5x5 conv1 (valid 3x3 on 7x7)
    \def\nC{3}  % 3x3 conv2 (valid 3x3 on 5x5)
    
    %-------------------------
    % Input grid (7x7)
    %-------------------------
    \begin{scope}[shift={(\sA,0)}]
      \draw[box] (0,0) rectangle (\nA,\nA);
      \foreach \i in {1,...,\nA} \draw[grid] (\i,0) -- (\i,\nA);
      \foreach \j in {1,...,\nA} \draw[grid] (0,\j) -- (\nA,\j);
      \node[above=2mm of {(3.5,7)}] {\textbf{Input}};
    \end{scope}
    
    %-------------------------
    % Conv1 feature map (5x5)
    %-------------------------
    \begin{scope}[shift={(\sB,2)}]
      \draw[box] (0,0) rectangle (\nB,\nB);
      \foreach \i in {1,...,\nB} \draw[grid] (\i,0) -- (\i,\nB);
      \foreach \j in {1,...,\nB} \draw[grid] (0,\j) -- (\nB,\j);
      \node[above=2mm of {(2.5,5)}] {\textbf{Conv1} (3$\times$3, s=1)};
    \end{scope}
    
    %-------------------------
    % Conv2 feature map (3x3)
    %-------------------------
    \begin{scope}[shift={(\sC,3.5)}]
      \draw[box] (0,0) rectangle (\nC,\nC);
      \foreach \i in {1,...,\nC} \draw[grid] (\i,0) -- (\i,\nC);
      \foreach \j in {1,...,\nC} \draw[grid] (0,\j) -- (\nC,\j);
      \node[above=2mm of {(1.5,3)}] {\textbf{Conv2} (3$\times$3, s=1)};
    \end{scope}
    
    %-------------------------
    % Highlight chosen unit in Conv2: center cell (2,2)
    %-------------------------
    \begin{scope}[shift={(\sC,3.5)}]
      \draw[rf1, line width=1pt] (1,1) rectangle (2,2);
      \node at (1.5,1.5) {\tiny \(\star\)};
    \end{scope}
    
    %-------------------------
    % Its 3x3 receptive patch in Conv1: cells (2..4, 2..4)
    % (exactly aligned with grid lines, no half-cell offsets)
    %-------------------------
    \begin{scope}[shift={(\sB,2)}]
      \draw[rf1] (1,1) rectangle (4,4);
    \end{scope}
    
    %-------------------------
    % The induced 5x5 receptive field in the Input: cells (2..6, 2..6)
    %-------------------------
    \begin{scope}[shift={(\sA,0)}]
      \draw[rf0] (1,1) rectangle (6,6);
      \node[lab, below right] at (6,1) {Receptive field};
    \end{scope}
    
    %-------------------------
    % Connector arrows with precise targets (centers)
    %-------------------------
    % centers: Input RF (3.5,3.5); Conv1 patch (2.5,4.5); Conv2 unit (1.5,5.0)
    \draw[->, thick, shorten >=2pt, shorten <=2pt]
      (\sA+3.5, 3.5) .. controls +(+3, +2.5) and +(-2.5, -0.5) .. (\sB+2.5, 4.5);
    
    \draw[->, thick, shorten >=2pt, shorten <=2pt]
      (\sB+2.5, 4.5) .. controls +(+3, +1) and +(-2, 0) .. (\sC+1.5, 5.0);
    
    % Minimal legends
    \node[lab, align=left] at (\sB+6, 4.6) {3$\times$3 window};
    \node[lab, align=left] at (\sA+8.5, 3.2) {5$\times$5 in input};
    
    % Tiny axis hints
    \node[lab] at (\sA+3.5, -1.3) {spatial $x$};
    \node[lab, rotate=90] at (\sA-0.9, 3.5) {spatial $y$};
    
    \end{tikzpicture}
    \captionsetup{hypcap=false}
    \captionof{figure}{The receptive field in a 2D convolutional network is shown.}
    \label{fig:receptive_field}
\end{center}

\subsection{Pooling and Downsampling}
To achieve \emph{translation invariance} and reduce spatial dimensionality, CNNs interleave convolutions with \emph{pooling} (e.g. max or average pooling). The pooling layer summarizes a set of adjacent nodes from the previous (convolutional) layer with a single value, and there are two common ways to get this value: \textbf{average-pooling} and \textbf{max-pooling}. 
\begin{itemize}
    \item Average-pooling takes the kernel $k = (1/l, \dots, 1/l)$ with size $l$ and convolutes with the previous layer, returning the average of all the nodes. This is useful for multiscale recognition.
    \item Max-pooling returns the maximal value from the $l$ input nodes, discarding everything else.
\end{itemize}
Pooling is a fixed layer and acts like a convolution layer as described above, reducing layer size, i.e. \textbf{downsmapling}.

\begin{eg}[Tensor Operations]
    In ML lingo, a tensor operation is the manipulation of high-dimensional arrays of numbers. If we use a $256 \times 256$-pixel RGB image as input example, with minibatch size $64$, then out input is a (fourth-rank) tensor of the shape $256 \times 256 \times 3 \times 64$. After convolution with $96$ kernels, each with size $5 \times 5 \times 3$ and stride $2$, the resulting tensor has the shape $128 \times 128 \times 96 \times 64$. 

    We call these $96$ kernels \textbf{channels}, and they form a \textbf{feature map} representing different information about some specific feature. In a very deep neural network (such as this one), later kernels will generally represent high-level features, such as contour or faces in a facial recognition program.
\end{eg}

\subsection{Batch Renormalization}
It is observed that by rescaling internal values of the CNN, we can improve the rate of convergence of SGD. The reason for this is not well-understood.

For learnt parameters $\beta$ and $\gamma$, which may be different for different nodes (so they are \textit{node-specific}), we will renormalize each output from the $i$-th node as
\begin{equation}
    z_i \longrightarrow \hat{z}_i = \gamma \frac{z_i - \mu}{\sqrt{\epsilon + \sigma^2}} + \beta,
\end{equation}
where $\mu = \mathrm{mean}(z)$, $\sigma = \mathrm{STD}(z)$, and $0 < \epsilon < 1$ is a small parameter to avoid division by zero.

\subsection{Regularization}
Regularization is a method used to limit the complexity of a model.

In the context of ANNs, regularization is normally implemented with \textbf{weight decay}. With weight decay, we penalize large weights, so the CNN must be more confident when choosing large weights. This prevents arbitrarily odd behavior that come from overfitting. To implement weight decay, we add the \textbf{penalty function}
\begin{equation}
    P(W) = \lambda \sum_{i, j} \left|{W_{ij}}\right|^2 = \lambda \Vert W\Vert^2
\end{equation}
to the loss function, i.e.
\begin{equation}
    \mathrm{Loss} \mapsto \mathrm{Loss} + P(W).
\end{equation}
Here $\lambda$ is a \textit{hyperparameter} usually chosen to be around $10^{-4}$.
\begin{note}
    When we choose a network architecture, the choice is usually arbitrary. For example, the number of layers and the number of nodes within each individual layer are usually not chosen based on some first principles. This imposes an absolute constraint on our hypothesis space. However, weight decay acts as a penalty, and therefore gives a softer constraint on the network.
\end{note}

\subsection{Dropout}
\textbf{Dropout} is a \textit{regularization technique} to increase the generalization power of the DNN model. During training, we deactivate a randomly chosen fraction $p$ of nodes and scale the surviving nodes accordingly, i.e. each node $z_i$ is either set to $z_i / p$ with probability $p$ or $0$ with probability $1-p$. Then, backpropagation is applied to the newly created neural network of expected size $pn$, where $n$ is the original size of the DNN. Refer to the diagram below for an illustration.

Empirically it has been observed that $p = 0.5$ works best for hidden layers, while $p=0.8$ works best for the input layer. Since dropout introduces noise during training, it makes the model more robust, hence better at generalizing to more data. The downside is of this is higher difficulty in training.

\begin{remark}
    \begin{enumerate}[(i)]
        \item The dropout scheme is similar to selection processes that guide gene evolution in nature.
        \item Applying dropout to later layers in the CNN forces the output to be produced robustly, since the network is trained to pay attention to all the high-level features in the input examples.
    \end{enumerate}
\end{remark}