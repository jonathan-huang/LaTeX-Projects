\chapter{Numerical Methods}
% \lecture{3}{10 Sep. 08:00}{Third Lecture}


\section{Runge-Kutta Methods}
\subsection{Classical Runge-Kutta Method}
The \textbf{Classical Runge-Kutta method} is a widely used technique for solving ordinary differential equations (ODEs). It is a single-step method, meaning that it uses information from the current time step to compute the solution at the next time step.

The method is based on the idea of approximating the solution by evaluating the derivative at several points within the interval and taking a weighted average of these slopes. The classical Runge-Kutta method is often referred to as the 4th-order method, as it achieves a local truncation error of \(O(h^5)\) and a global error of \(O(h^4)\).

The method can be summarized in the following steps:
\begin{enumerate}
    \item Given an initial value problem of the form
   \[
   \frac{dy}{dt} = f(t, y), \quad y(t_0) = y_0,
   \]
   we want to compute \(y(t_0 + h)\) for a small step size \(h\).
   \item Compute the following intermediate slopes:
   \begin{equation}
        \begin{split}
            k_1 &= h f(t_n, y_n), \\
            k_2 &= h f\left(t_n + \frac{1}{2}h, y_n + \frac{1}{2}k_1\right), \\
            k_3 &= h f\left(t_n + \frac{1}{2}h, y_n + \frac{1}{2}k_2\right), \\
            k_4 &= h f\left(t_n + h, y_n + k_3\right).
        \end{split}
    \end{equation}
    \item Update the solution using a weighted average of the slopes:
   \begin{equation}
       y_{n+1} = y_n + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4).
   \end{equation}
\end{enumerate}

\subsection{Runge-Kutta-Fehlberg Method}
The \textbf{Runge-Kutta-Fehlberg (RKF) method} , also known as simply the Fehlberg method, is a popular adaptive step size method. It estimates the local truncation error and adjusts the step size accordingly.

It can be formulated as either a fourth-order or fifth-order method, and is particularly useful for solving ordinary differential equations where the solution may change rapidly. The recursive solutions are 
% Fehlberg 4th-order solution
\begin{equation}
    y_{n+1}^{(4)} = y_n + \frac{25}{216}k_1 + \frac{1408}{2565}k_3 + \frac{2197}{4104}k_4 - \frac{1}{5}k_5
\end{equation}
and
\begin{equation}
    y_{n+1}^{(5)} = y_n + \frac{16}{135}k_1 + \frac{6656}{12825}k_3 + \frac{28561}{56430}k_4 - \frac{9}{50}k_5 + \frac{2}{55}k_6,
\end{equation}

where the \( k_i \) values are defined as follows:
\begin{equation}
    \begin{split}
        k_1 &= h\, f(t_n, y_n), \\
        k_2 &= h\, f\!\left(t_n + \frac{1}{4}h y_n + \frac{1}{4}k_1 \right), \\
        k_3 &= h\, f\!\left(t_n + \frac{3}{8}h y_n + \frac{3}{32}k_1 + \frac{9}{32}k_2 \right), \\
        k_4 &= h\, f\!\left(t_n + \frac{12}{13}h y_n + \frac{1932}{2197}k_1 - \frac{7200}{2197}k_2 + \frac{7296}{2197}k_3 \right), \\
        k_5 &= h\, f\!\left(t_n + h y_n + \frac{439}{216}k_1 - 8k_2 + \frac{3680}{513}k_3 - \frac{845}{4104}k_4 \right), \\
        k_6 &= h\, f\!\left(t_n + \frac{1}{2}h y_n - \frac{8}{27}k_1 + 2k_2 - \frac{3544}{2565}k_3 + \frac{1859}{4104}k_4 - \frac{11}{40}k_5 \right).
    \end{split}
\end{equation}

The Butcher tableau for the Fehlberg 4(5) method (RKF45) is shown in table \ref{tab:butcher_fehlberg}.

\begin{table}[h]
\centering
\setlength{\tabcolsep}{10pt}   % extra column spacing
\renewcommand{\arraystretch}{2.3} % extra row spacing
\begin{tabular}{c|cccccc}
$c_i$ & $a_{i1}$ & $a_{i2}$ & $a_{i3}$ & $a_{i4}$ & $a_{i5}$ & $a_{i6}$ \\
\hline
0     &           &           &           &           &           &           \\
$\dfrac{1}{4}$ & $\dfrac{1}{4}$ \\
$\dfrac{3}{8}$ & $\dfrac{3}{32}$ & $\dfrac{9}{32}$ \\
$\dfrac{12}{13}$ & $\dfrac{1932}{2197}$ & $-\dfrac{7200}{2197}$ & $\dfrac{7296}{2197}$ \\
1     & $\dfrac{439}{216}$ & $-8$ & $\dfrac{3680}{513}$ & $-\dfrac{845}{4104}$ \\
$\dfrac{1}{2}$ & $-\dfrac{8}{27}$ & $2$ & $-\dfrac{3544}{2565}$ & $\dfrac{1859}{4104}$ & $-\dfrac{11}{40}$ \\
\hline
& $\dfrac{25}{216}$ & 0 & $\dfrac{1408}{2565}$ & $\dfrac{2197}{4104}$ & $-\dfrac{1}{5}$ & 0 \\
& $\dfrac{16}{135}$ & 0 & $\dfrac{6656}{12825}$ & $\dfrac{28561}{56430}$ & $-\dfrac{9}{50}$ & $\dfrac{2}{55}$ \\
\end{tabular}
\caption{Butcher tableau for the Runge--Kutta--Fehlberg 4(5) method.}
\label{tab:butcher_fehlberg}
\end{table}

\section{Automatic Differentiation}
Three main methods of calculating derivatives are: numeric differentiation, symbolic differentiation, and \textbf{automatic differentiation (AD)}. Automatic differetiation is a powerful technique that allows for the efficient and accurate computation of derivatives of functions, particularly in the context of machine learning and optimization.

Numeric differentiation approximates derivatives using finite differences, which can introduce numerical errors. Symbolic differentiation manipulates mathematical expressions to derive exact formulas, this solves the issue of numerical inaccuracies and instabilities but can be computationally expensive for complex functions. The leading issue for symbolic differentiation is \textit{expression swell}, where the size of the expression grows exponentially with the number of operations, making it impractical for large functions.

\begin{eg}[Expression in Composite Functions]
    The function 
    \begin{equation}
        f(x)=\frac{e^{wx+b}+e^{-(wx+b)}}{e^{wx+b}-e^{-(wx+b)}}
    \end{equation}
    has a symbolic derivative of 
    \begin{equation}
        \frac{\partial f}{\partial w} = \frac{(-x e^{-b-wx}-x e^{b+wx})(e^{-b-wx}+e^{b+wx})}{(-e^{-b-wx}+e^{b+wx})^{2}} + \frac{-x e^{-b-wx}+x e^{b+wx}}{-e^{-b-wx}+e^{b+wx}}.
    \end{equation}
\end{eg}

Automatic differentiation, on the other hand, computes derivatives by applying the chain rule to the operations in a function, allowing for efficient and accurate derivative calculations.

\section{Adjoint Method in Optics Design}
The nonlinear adjoint method can be applied to general (nonlinear) objective functions and sources of nonlinearity, allowing for the design of novel nonlinear optical devices. 

The goal of inverse design is to find a set of real-valued design variables that maximize a real-valued objective function $ \mathcal{L} = \mathcal{L} (\bvec{E}, \bvec{E}^{\star}, \bvec{\phi} ) $. In linear systems, $ \bvec{E} $ is a complex-valued vector corresponding to the electric field solution of the (linear) Maxwell equations. In the nonlinear case, $ \bvec{E} $ may be interpreted more generally as the solution to a nonlinear equation 
\begin{equation}
    \label{equ:nonlinear}
    F(\bvec{E}, \bvec{E}^{\star}, \bvec{\phi}) = 0. 
\end{equation}

The solution to equation (\ref{equ:nonlinear}) can be found with the Newton-Raphson method, which has a complexity of $\mathcal{O}(N^2)$, where $ N $ is the number of design variables $ \operatorname{dim}(\phi) $.


\section{Stochastic Differential Equations}
The state of the art method for solving quantum noise problems is through solving \textbf{stochastic differential equations (SDEs)} . The SDEs are often used to model systems influenced by random noise, such as quantum systems subject to thermal fluctuations or other forms of stochastic perturbations.