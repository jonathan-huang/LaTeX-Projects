\subsection{向量的介紹}

\begin{mydef}[生成向量]
    我們說一個集合\textbf{生成} 一個空間，當且僅當這個集合裡的向量可以組合出這個空間裡所有的向量。也就是說，如果我們有一個向量空間 \(V\)，和一組向量 \( \{ \bvec{v_1}, \bvec{v_2}, \dots , \bvec{v_n} \} \)，對於 \(V\) 裡的任意向量 \( \bvec{v} \)，都存在一組係數 \( c_1, c_2, \dots , c_n \)，使得
    \begin{equation}
        \bvec{v} = c_1 \bvec{v_1} + c_2 \bvec{v_2} + \cdots + c_n \bvec{v_n}.
    \end{equation}
    我們可以把 \( v_1, v_2, \dots, v_n \) 叫做\textbf{生成向量}。
\end{mydef}

\begin{mydef}[線性獨立]
    前面我們將基底看成是可以生成整個空間裡所有向量的「最小」集合。這個最小的概念要怎麼用數學嚴格地定義呢？我們可以把這個定義推廣到更一般的情況：如果我們有一組向量 \( \{ \bvec{v_1}, \bvec{v_2}, \dots , \bvec{v_n} \} \)，我們可以說這組向量是\textbf{線性獨立}的，如果我們沒有辦法找到一組不全為零的係數 \( c_1, c_2, \dots , c_n \)，使得
    \begin{equation}
        c_1 \bvec{v_1} + c_2 \bvec{v_2} + \cdots + c_n \bvec{v_n} = \bvec{0},
    \end{equation}
    也就是說這些向量沒有辦法被彼此組合出來。前面提到的「最小集合」意思就是這個集合裡的向量們是線性獨立的。
\end{mydef}

\begin{problem}[向量的線性獨立]
    ~
    以下這些向量的集合是線性獨立的嗎？
    \begin{enumerate}
        \item \( \bvec{v_1} = (1, 0, 0), \bvec{v_2} = (0, 1, 0), \bvec{v_3} = (0, 0, 1) \) 
        \item \( \bvec{v_4} = (1, 1, 0), \bvec{v_5} = (1, 0, 1), \bvec{v_6} = (0, 1, 1) \)
        \item \( \bvec{v_7} = (1, 1, 1) \)
    \end{enumerate}
\end{problem}

\subsection{方陣對角化}

\begin{mydef}[可逆矩陣 invertible matrix]
    我們說一個方陣 \(P\) 是\textbf{可逆矩陣}，當且僅當存在另一個方陣 \(P^{-1}\)，使得
    \begin{equation}
        P P^{-1} = P^{-1} P = I,
    \end{equation}
    其中 \(I\) 是單位矩陣。這個矩陣 \(P^{-1}\) 稱為 \(P\) 的\textbf{反矩陣}。
\end{mydef}

\begin{mydef}[對角矩陣 diagonal matrix]
    我們說一個方陣 \(D\) 是\textbf{對角矩陣}，當且僅當它的非對角線元素全為零。也就是說，對於一個 \(n \times n\) 的矩陣 \(D\)，如果
    \begin{equation}
        D_{ij} = 0 \quad \text{當 } i \neq j,
    \end{equation}
    那麼 \(D\) 就是對角矩陣。
\end{mydef}

給定一個 $n\times n$ 方陣 $A$。若存在可逆矩陣 $P$ 與對角矩陣 $D$ 使得
\begin{equation}
    A = P D P^{-1},
    \label{eq:diag-def}
\end{equation}
則稱 $A$ \textbf{可對角化}（diagonalizable），而這個過程稱為\textbf{（矩陣）對角化}。符合方程式 \eqref{eq:diag-def} 的兩個方陣 $ A $、$ D $也被稱作\textbf{相似矩陣}。等價地，\eqref{eq:diag-def} 左右同乘可得
\begin{equation}
    P^{-1} A P = D
    \label{eq:similarity}
\end{equation}
所以我們可以把一個可對角化的矩陣寫成對角的形式。計算對角化時，一個很重要的概念叫做矩陣的特徵值與特徵向量。以下我們給出一個簡單的定義。

\begin{mydef}[特徵值與特徵向量 eigenvalues and eigenvectors]
    給定一個 $n\times n$ 方陣 $A$，如果存在一個純量 $\lambda$ 與非零向量 $v$ 使得
    \begin{equation}
        A v = \lambda v,
        \label{eq:eigen-def}
    \end{equation}
    那麼我們稱 $\lambda$ 為 $A$ 的一個\textbf{特徵值}（eigenvalue），而 $v$ 為對應於 $\lambda$ 的\textbf{特徵向量}（eigenvector）。
\end{mydef}

\subsubsection{為什麼對角化有用？}

若 $A=PDP^{-1}$，則許多運算會變得簡單，尤其是矩陣的高次方：
\begin{equation}
    A^k=(PDP^{-1})^k = P D^k P^{-1},\qquad k\in \mathbb{N}.
\end{equation}
而 $D^k$ 只需將對角線元素各自取冪即可。

\begin{example}[對角矩陣的高次方 $D^k$ 很好算]
    ~

    若
    \[
    D=\begin{pmatrix}3&0\\0&-2\end{pmatrix},
    \]
    則
    \[
    D^5=\begin{pmatrix}3^5&0\\0&(-2)^5\end{pmatrix}
    =
    \begin{pmatrix}243&0\\0&-32\end{pmatrix}.
    \]
\end{example}

\subsubsection{可對角化的核心判準（觀念版）}

對角化的核心事實是：
\begin{quote}
$A$ 可對角化 $\Longleftrightarrow$ $A$ 能找到 $n$ 個\textbf{線性獨立}的特徵向量。
\end{quote}

更具體地，若你能找到一組基底 $v_1,\dots,v_n$（全為特徵向量），滿足
\[
A v_i = \lambda_i v_i,
\]
則令
\[
P=\begin{pmatrix} 
    v_1 & v_2 & \cdots & v_n 
\end{pmatrix},
\qquad
D=\mathrm{diag}(\lambda_1,\dots,\lambda_n) \equiv \begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_n
\end{pmatrix},
\]
便可得到 $A=PDP^{-1}$。

\subsubsection{實作流程}

\begin{enumerate}[label=(\arabic*), start=0]
    \item 確認 $A$ 是 $n\times n$ 方陣。
    \item 計算矩陣 $ A $ 的特徵多項式 $\chi_A(\lambda)=\det(A-\lambda I)$，解特徵多項式的根 $\chi_A(\lambda)=0$ 得到特徵值。
    \item 對每個特徵值 $\lambda_i$，解
    \[
    (A-\lambda_i I)v=0
    \]
    得到一組特徵向量。
    \item 檢查是否能收集到 $n$ 個線性獨立特徵向量；若不足則不可對角化。
    \item 令 $P$ 的欄向量依序為你選到的特徵向量 $v_1,\dots,v_n$；令
    \[
    D=\mathrm{diag}(\lambda_1,\dots,\lambda_n)
    \]
    且 $\lambda_i$ 與 $v_i$ 的順序一致。
    \item 寫出 $A=PDP^{-1}$，並可選擇驗算 $P^{-1}AP=D$。
\end{enumerate}

\begin{example}[例題：可對角化（不同特徵值）]
    ~

    \noindent 令
    \[
    A=\begin{pmatrix}
    4&1\\
    0&2
    \end{pmatrix}.
    \]

    \begin{enumerate}[(1)]
        \item 特徵值： 
        \[
        \chi_A(\lambda)=\det\begin{pmatrix}4-\lambda&1\\0&2-\lambda\end{pmatrix}
        =(4-\lambda)(2-\lambda),
        \]
        故特徵值為 $\lambda_1=4,\ \lambda_2=2$。

        \item 特徵向量：
            \begin{itemize}
        \item $\lambda=4$：解 $(A-4I)v=0$：
        \[
        A-4I=\begin{pmatrix}0&1\\0&-2\end{pmatrix}
        \Rightarrow y=0,\ x\ \text{自由}
        \Rightarrow v_1=\begin{pmatrix}1\\0\end{pmatrix}.
        \]
        \item $\lambda=2$：解 $(A-2I)v=0$：
        \[
        A-2I=\begin{pmatrix}2&1\\0&0\end{pmatrix}
        \Rightarrow 2x+y=0
        \Rightarrow v_2=\begin{pmatrix}1\\-2\end{pmatrix}.
        \]
        \end{itemize}

        \item 組出 $P$ 與 $D$：
        \[
        P=\begin{pmatrix}1&1\\0&-2\end{pmatrix},
        \qquad
        D=\begin{pmatrix}4&0\\0&2\end{pmatrix}.
        \]
        因此 $A=PDP^{-1}$（可自行驗算 $P^{-1}AP=D$）。
    \end{enumerate}
\end{example}

\begin{example}[例題：重根但仍可對角化]
    ~

    \noindent 令 $A=2I$：
    \[
    A=\begin{pmatrix}
    2&0\\
    0&2
    \end{pmatrix}.
    \]
    其特徵多項式為 $(2-\lambda)^2$，特徵值 $\lambda=2$ 為重根。但 $A-2I=0$，所以任意非零向量皆為特徵向量。我們可以輕易找到兩個線性獨立的特徵向量，例如 $e_1=(1,0)^T$ 與 $e_2=(0,1)^T$。因為湊滿 $n=2$ 個線性獨立特徵向量，因此可對角化（事實上它本來就已是對角矩陣）。
\end{example}

\begin{example}[例題：重根且不可對角化]
    ~

    \noindent 令
    \[
    A=\begin{pmatrix}
    1&1\\
    0&1
    \end{pmatrix}.
    \]
    \[
    \chi_A(\lambda)=\det\begin{pmatrix}1-\lambda&1\\0&1-\lambda\end{pmatrix}=(1-\lambda)^2,
    \]
    故唯一特徵值 $\lambda=1$ 為重根。

    再看特徵向量，解 $(A-I)v=0$：
    \[
    A-I=\begin{pmatrix}0&1\\0&0\end{pmatrix}
    \Rightarrow y=0,\ x\ \text{自由}
    \]
    所有的特徵向量都必須是 $\begin{pmatrix}1\\0\end{pmatrix}$ 的倍數。
    我們無法找到兩個線性獨立的特徵向量，所以 \textbf{$A$ 不可對角化}。
\end{example}